# 统计学基础
Hi，同学们，上一阶段我们学习了Numpy的基本操作、数据分析的基本流程、Pandas在数据分析各个过程中的应用以及Matplotlib&Pandas的可视化基础，截至目前，你们已经算是掌握了基础的数据分析技能啦！撒花！但是在统计学理论和预测方面仍有欠缺，那么在本阶段我们就来补上！
 
我们分两篇文章来介绍数据分析相关的基础概率论与统计学知识：
- 上篇：描述统计学、概率与贝叶斯法则、常见概率分布
- 下篇：推论统计学、置信区间、假设检验、线性回归、逻辑回归
 

 
 
 
## 描述统计学基础
 
描述统计分析就是通过数字或可视化的方法，对数据集进行整理、分析，并对数据的分布状态、数字特征和随机变量之间的关系进行估计和描述。其简要可以分为集中趋势分析、离散程度分析以及相关分析三大部分。
 
### 数据类型
 
数据类型是基础，尤其是之后在进行回归预测时，针对不同的数据类型可以选择不同的算法，所以必须掌握。
 
数据类型可以分为两大类：数值和分类；进而分为四小类：连续、离散、定序和定类。
 
| **数据类型** |                        |                                      |
| ------------ | ---------------------- | ------------------------------------ |
| **数值:**    | **连续**               | **离散**                             |
|              | 身高、年龄、收入       | 书中的页数、院子里的树、咖啡店里的狗 |
| **分类:**    | **定序**               | **定类**                             |
|              | 字母成绩等级、调查评级 | 性别、婚姻状况、早餐食品             |
 
### 描述统计的量
 
| 数据类型  | 描述方面           | 描述方式                    | 备注                                 |
| --------- | ------------------ | --------------------------- | ------------------------------------ |
| **数值:** | **集中趋势**       | 均值                        |                                      |
|           |                    | 中位数                      | 偶数个时取中间两值均数               |
|           |                    | 众数                        | 存在没有或多个的可能                 |
|           | **离散程度**       | 极差                        | max - min                            |
|           |                    | 四分位差（IQR）             | 75%数 - 25%数                        |
|           |                    | 方差                        | 每个观察值与均值之差平方和的平均数   |
|           |                    | 标准差                      | 方差的平方根                         |
|           | **数据形状**       | 左偏态                      | 均值小于中位数（普遍但不绝对，下同） |
|           | （需做直方图）     | 右偏态                      | 均值大于中位数                       |
|           |                    | 对称分布（通常是正态分布）  | 均值等于中位数                       |
|           | **异常值**         | 一般为上下超过1.5倍四分位差 | 处理方式见下面【异常值的处理】       |
| **分类:** | 分类计量个数或比例 |                             |                                      |
 
- 偏态分布示意图
 
  ![](https://tse4.mm.bing.net/th?id=OIP.eJMf4CCd_ylkXqyGPD0NbQHaE8&pid=Api)
 
- 其他相关概念：
 
  - 五数概括描述法：利用**最小值、第一四分位数（25%处）、第二四分位数（中位数）、第三四分位数（75%处）和最大值**五个数对**数值型**变量的**离散程度**进行描述的方法。
 
  - 当我们的数据遵循**正态**分布时，我们可以使用`均值`和`标准差`描述我们的数据集。        
 
    但是，如果我们的数据集是**偏态**分布，`五数概括法`（和关联的集中趋势度量）更适用于概括数据。
 
  - 除直方图外，你还可以使用**箱线图**进行统计描述，箱线图其实是五数概括法的可视化。
 
- 异常值的处理：
 
  **1.** 注意到它们的存在并确定对统计的影响，可以通过绘制直方图或箱线图的方法直观观察，也可以使用五数概括法（对应于Pandas中的describe函数）        
 
  **2.** 如果是输入错误 — 删除或改正        
 
  **3.** 理解它们为何存在，以及对我们关注数据问题的影响。        
 
  **4.** 当有异常值时，使用五数概括法的值通常能比均值和标准差等度量更好地体现异常值的存在。           
 
### 辛普森悖论
 我们先举个例子🌰，如下所示是我编的院校录取数据。
 
 |专业|男生申请|男生录取|男生录取率|女生申请|女生录取|女生录取率|
 |---|---|---|---|---|---|---|
 A|800|400|50%|200|150|75%|
 B|200|20|10%|800|160|20%|
 合集|1000|420|42%|1000|310|31%|
 
 从上表中我们很明显能看出，如果只比较A专业或者B专业的话，女生的录取率均高于男生，说明女生更受青睐吗？如果我们看整个院校两专业合计的话，男生的录取率又比女生高了，那这又说明男生更受青睐吗？
 如上这种悖论便是**辛普森悖论**，辛普森悖论是在某个条件下的两组变量，分别讨论时都会满足某种性质，可是一旦合并考虑，却可能导致相反的结论。
 造成这种现象的原因便是**混杂因素**，混杂因素就是一个与核心研究无关的变量，它会随着变量的改变而改变，就比如说在如上的例子中，不同专业的总人数就有很大差异，它会随着专业的改变而改变，正是由于改变量的变化才导致了悖论的出现。
 这里不必深究，只是为了提醒大家要**以多种方式去观察数据**。在之后处理类似问题时就要进行多变量分析，查看是否存在潜在因素，这样才能帮助我们认清事件的本质。
 
## 概率
 
### 概率与统计的关系
 
在介绍概率之前，我们先来看下概率与统计之间的关系：
![iAkgoj.png](https://s1.ax1x.com/2018/09/12/iAkgoj.png)
（图像来源：Udacity）
概率是由模型（MODEL）去预测数据（DATA），而统计是由数据去建立模型（进而再去做预测）。
 
### 基础知识


**概率**，是一种几率、可能性，描述是事件发生的可能性度量量。**随机事件**，指一个被赋予机率的事件集合，针对的是事件在样本空间的一个子集。事件A发生的概率，用符号P(A)表示 。
 
- 任何事件的发生概率在 0 和 1 之间，其中包括 0 和 1。（0表示不可能发生，1表示必然发生）
 
- 独立事件：事件A与事件B是否发生/发生的结果没有任何关系，就可以说事件A与B互为独立事件。比如说，第一次掷骰子的结果与第二次的结果
 
- 互斥事件：不可能在同一次实验中出现的俩事件。比如说，掷骰子实验中的1和6
 
- 对立事件：是一种特殊的互斥事件，即试验中只有俩种可能A和B，那么事情的发生非A即B。可以表示为
 
  ![iuFI3Q.png](https://s1.ax1x.com/2018/09/22/iuFI3Q.png)
 
  如掷硬币的正面和反面。
 
- 加法原理：若两种方法均能完成此事，则此事发生的概率为P(A) + P(B)
- 乘法原理：若两个步骤分别不能完成此事，则此事发生的概率为P(A)·P(B)
 
### 两种常见的离散型分布
#### 二项分布
 
也叫伯努利分布，指n个独立的事件A发生的概率分布。设每次试验中事件A发生的概率为p，则进行n次试验，A发生k次的概率为：
 
![iuFocj.png](https://s1.ax1x.com/2018/09/22/iuFocj.png)
如检查某产品，有n个产品合格的概率。

#### 泊松分布

如果某事件以固定强度$\lambda(\lambda>0)$随机且独立的出现，该事件在单位时间/单位面积内出现的次数（个数）就可以看作是泊松分布。它的表达式为：
$$P(X=k)=\frac{\lambda^{k}e^{-\lambda}}{k!},k=0,1,2,...$$
如某人一天内收到的微信数量；一个月内顾客的购买次数等等。

### 条件概率
 
在现实中，我们处理的事情并不像骰子和硬币那样简单，有些时间的结果往往依赖于其他的事情，比如说晨练的概率跟这个人是不是夜猫子有关等等。那么，这就引出了条件概率，即在事件B发生的条件下，事件A发生的概率为：
 
![iuFTjs.png](https://s1.ax1x.com/2018/09/22/iuFTjs.png)
 
其中，$P(AB)$表示$AB$同时发生的概率。
 
我们可以使用文氏图来帮助我们理解事件之间的关系，如下图中，AB同时发生的概率可以表示为两个圆的交集，那么B已经发生的条件下A发生的概率就是这个交集（橙色部分）占整个B圆的比例。
 
![iA5UMT.png](https://s1.ax1x.com/2018/09/13/iA5UMT.png)
 
> 之前讲过独立事件，那么用公式的方式可以表达为：`P(A) = P(A|B)`。根据条件概率公式可以推导出，当`P(AB) = P(A)P(B)`时，则可说明A事件与B事件相互独立。
 
**全概率公式**
 
也就是A发生的概率为在互斥的多个事件（B1，B2...）已发生的条件下的条件概率之和。公式可以表示为：
 
![iuFHun.png](https://s1.ax1x.com/2018/09/22/iuFHun.png)
 
 
### 贝叶斯法则
 
贝叶斯法则是概率推理的黄金法则，是利用先验概率计算后验概率的方法。
 
我们还是通过一个例子来阐述，假设某医院新研究了一种检验患者是否患癌的技术，临床测试数据如下：该技术具有5%的假阳性及3%的假阴性，现已知某一群体的患癌率为0.005，问该技术是否可以应用于普查。

我们设事件Pos={检查结果为阳性}，事件C={检查者患癌}，那么如上的临床数据可以写成：

$$P(Pos|\bar{C}) = 5\% $$
$$P(\bar{Pos}|C) = 3\%$$
 
而我们的目的是确认该技术是否可以用于普查，也就是检查结果为阳性时的患癌率（真阳性），也就是$P(C|Pos)$，这个值越高，该检查就越适合普查.

由已知条件$P(\bar{Pos}|C) = 3\%$，可以计算出：
$$P(Pos|C) = 1 - P(\bar{Pos}|C) = 0.97$$
由条件概率可知：
$$P(Pos,C) = P(C)\cdot P(Pos|C)$$
$$P(Pos,\bar{C}) = P(\bar{C})\cdot P(Pos|\bar{C})$$
则：
$$P(Pos) = P(Pos,C)+P(Pos,\bar{C})$$
则：
$$P(C|Pos) = \frac{P(Pos,C)}{P(Pos)}$$
代入各值，得到结果约为8.88%，也就是说，若该检查用于普查的话，每100个阳性患者中只有不到9个人是患癌的，还需要再进一步的检查才能确诊，所以该检查并不能用于癌症的普查。
> 🤔那如果群体的患癌率比较高，约为90%，那么检查结果为阳性的患癌率有多少呢？这又说明了什么？

在这个例子中：
 
癌症发生的概率`P(C)`为**先验概率**，即在我们进行检查之前就对患癌概率的一个判断；阳性结果下为癌症的概率`P(C|Pos)`为**后验概率**（阳性下非癌症、阴性癌症、阴性非癌症都是），这些是在检查发生之后，我们对患癌概率这件事的重新评估。
 
**这就是贝叶斯法则的含义。我们先预估一个"先验概率"，然后加入实验结果，由此得到更接近事实的"后验概率"。**
 
> 如果感觉理解困难，可以看一下白话版的贝叶斯讲解：[怎样用非数学语言讲解贝叶斯定理（Bayes's theorem）](https://www.zhihu.com/question/19725590)
 
## Python在概率中的应用
 
前面我们了解了概率的基本知识，本节就是利用Python去实施，应用的第三方包为NumPy和Pandas。
 
### 均匀随机取整
 
使用的函数为`numpy.random.randint(low, high=None, size=None, dtype='l')`：
 
- low：当没有high参数输入时，作为取值范围的最大值+1；当有high参数输入时，则作为取值范围的最小值；
- high：取值范围的最大值+1，默认为无输入；
- size：输入数字则表示取值的数量，输入元组则表示取值矩阵的行和列；
- dtype：数据类型，默认为np.int；
- 函数的输出为int或者是由int数据类型组成的ndarray。
 
函数的具体用法如下：
 
```python
import numpy as np
 
# 未定义high参数，取值范围是是0到1（即low - 1）
>>> np.random.randint(2, size=10)
array([1, 0, 0, 0, 1, 1, 0, 0, 1, 0])
# 定义了high参数，取值范围是2（即low）到4（即high - 1）
>>> np.random.randint(2,5,size = 10)
array([2, 2, 3, 3, 4, 2, 3, 2, 2, 4])
# 定义一个2x4的矩阵，取值范围是0到4
>>> np.random.randint(5, size=(2, 4))
array([[4, 0, 2, 1],
       [3, 2, 2, 0]])
```
 
### 不均匀随机取数
 
使用的函数为**numpy.random.choice**(*a*, *size=None*, *replace=True*, *p=None*):
 
- a：输入列表时，取数就从该列表中取；若输入为数字时，取值范围为0到该数字 - 1；
- size：输入数字则表示取值的数量，输入元组则表示取值矩阵的行和列；
- replace：布尔值，默认为True，若设置为False表示取数不会重复；（以从袋子中取球为例，True是取完放回再取，而False则是取了不放回，继续取）
- p：表示概率，与a的输入值一一对应。
- 函数的输出为int或者是由int数据类型组成的ndarray。
 
函数的具体用法如下：
 
```python
#输入数字时取数
>>> np.random.choice(5, 3)
array([0, 3, 4])
#输入列表时取数
>>> np.random.choice([0,1], 3)
array([0, 1, 1])
#设置replace参数
>>> np.random.choice(5, 3, replace=False)
array([3,1,0])
#设置概率
>>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])
array([2, 3, 0])
```
 
### 二项分布
 
使用的函数为**numpy.random.binomial**(*n*, *p*, *size=None*)，参数也很好解释。
 
参数中的n即为每次试验中取值的次数，p则为试验中的某一种事件成功的概率，size则是试验的次数。
 
### 条件概率与贝叶斯规则测试
 
这里主要是一些Pandas函数的应用，经过上一阶段的学习应该已经很熟练了。
 
主要涉及的函数是分组和统计计数类的函数，比如说groupby，query，count等，如果忘记的话，自己查官方文档或者之前的笔记，这里不再赘述。
 
## 正态分布
 
 在相同条件下，我们随机地对某一测试对象（抛硬币20次，其中正面的次数）进行多次测试（抛很多次20次）时，测得的数值在一定范围内波动（从0到20），其中接近平均值的数据（10左右）占多数，远离平均值的占少数。具有这种分布规律的随机变量的分布就称作正态分布。
 
大概长这样儿：
 
![iEkWCR.png](https://s1.ax1x.com/2018/09/14/iEkWCR.png)
 
它的概率密度函数可以表示为：
 
![iuFXNT.png](https://s1.ax1x.com/2018/09/22/iuFXNT.png)
 
其中$\mu$为均值，$\sigma$为标准差。
这里为什么要引入正态分布呢？
 
- 假设检验是基于正态分布的
- 许多社会和经济现象对应的随机变量分布，都可以用正态分布来描述
 
### 中心极限定理
 
先回顾下推论统计的几个概念：
 
1. **总体** —— 我们想要研究的整个群体。        
2. **参数** —— 描述总体的数值摘要        
3. **样本** —— 总体的子集        
4. **统计量** —— 描述样本的数值摘要        
 
依然举个例子先：我们想了解国内在校大学生关注AI派的比例，但因为大学生数量庞大，我们没办法去一个一个的做调查，所以我们随机的从大学生中抽出一部分群体，然后调查抽出部分的关注率，以此来估计全国大学生的关注率。
 
那例中的所有在校大学生就叫做**总体**，总体的关注率就是**参数**；我们随机抽出的那部分就叫做**样本**，其中包含的学生数量就叫做**样本容量**，在抽出的每组样本中我们还统计了关注率，这就是**统计量**。
 
> 一般的，样本数≥30即可称为大样本。 大样本条件下，抽样平均数的分布接近正态分布。
>
> 但必要抽样数目的确定是有相关公式计算的，这里就不给出了，感兴趣的话可以去搜搜看。
 
现在我们利用Python去模拟如上的抽样过程，看看会发生什么。
```python
#假设总体数量为十万，关注率为0.3，用1表示关注，可以模拟出总体的分布如下
>>>population = np.random.choice([0,1],size=100000,p=[0.7,0.3])
>>>print(f'Mean:{population.mean()}, Std:{population.std()}')

Mean:0.30114, Std:0.4587534200417475
```
为了方便后面调用，这里我们写了一个随机抽样&可视化的函数如下所示：
```python
#抽样&绘图
def sample_plot(sample_size):
    #随机抽样一万次，计算关注率
    rate_list = []
    for _ in range(10000):
        sample = np.random.choice(population,sample_size)
        rate = sample.mean()
        rate_list.append(rate)
    #可视化结果
    sns.distplot(rate_list)
    print(f'Mean:{np.mean(rate_list)}, Std:{np.std(rate_list)}')
```

我们先取样本容量为5，查看其关注率的分布情况如下：
```python
#样本容量为5时的结果
>>>sample_plot(5)
```
![KpYIV1.png](https://s2.ax1x.com/2019/10/14/KpYIV1.png)

 
（完全看不出这是个什么分布）
 
将样本容量改为20，我们再来看下结果：
![KptoLj.png](https://s2.ax1x.com/2019/10/14/KptoLj.png)
(已经有正态分布的亚子了）
最后我们把样本容量改为50:
![K9xQTf.png](https://s2.ax1x.com/2019/10/15/K9xQTf.png)
 
想比上一幅图，这幅可视化更接近正态分布。
> 🤔除了直方图外，观察下三次实验的均值与标准差和总体的均值与标准差，它们之间有什么关系呢？
 
上面我们的这个模拟过程就是**中心极限定理**的含义，随着样本容量的逐渐增大，比例的抽样分布越接近正态分布（但也不一定必须要很大很大才能近似于正态分布），同样这也适用于求和，平均数等，但**不适用于所有的统计量**，比如说最大值，方差等等。

> 上述模拟过程中的抽样方法叫做自助法(Bootstrap)，是一种从给定数据集中有放回的均匀抽样。

**中心极限定理的妙处就在于，我们可以从任意的乱七八糟的分布取任意数量的样本值，然后计算样本的均值（或者和），不断得取值求均，最终做他们频率的可视化，你会发现这是一个非常完美的正态分布。**
 
现实生活中有很多的随机过程，有的分布就是乱七八糟，但是你可以通过中心极限定理，得到他们均值或者和的正态分布，这也是为什么正态分布在统计中如此常用的原因之一。
 
> 如果感觉理解起来还是有点儿困难的话，你可以戳[在线抽样分布模拟器](http://1t.click/aAQ9)，自己动手试一试。
 
### 大数定理
 
大数定理表达的是**随着样本容量增加，样本平均数越来越接近总体平均数**，字面上的意思很好理解，但这里有一点要注意，我们举例来说明一下：
 
比如说，我现在有100枚硬币，放在一个盒子里，随便摇一下盒子，打开，对正面朝上的硬币进行计数（当然，我们知道期望为100 x 0.5 = 50）：
 
第一次实验的结果是55；第二次是60；第三次是70，三次实验的均值为（(55+60+70)/3 ≈62），那你觉得，下次实验的结果是更有可能小于50还是大于50呢？
 
你有可能这样想，根据大数定理，随着我们试验次数的不断增加，均值肯定是不断趋向于50的，前三次的实验中每次都超过50，那么下次的实验会有更大的可能小于50，来纠正前三次实验的偏差。
 
如果你真的这样想，你就陷入了**赌徒悖论**。大数定理不关心前面发生的有限次实验，因为后面还有无限次的实验，而这无限次实验的期望值是50。这个例子可能比较随意，但这就是大数定理的含义。

 
## 最后
 
本周主要对描述统计学和概率的基础知识进行了总结，这部分偏理论一些，如果觉得理解起来有点吃力，可以去网上搜集一些资料或者找一些教科书去查阅，要求是：不一定要完全掌握其原理，但求理解和会用。
**额外参考资料**：
  - [【课程】可汗学院-概率论与统计](http://1t.click/aAQ3)
  - [【教材】机会的数学-陈希孺](http://1t.click/aAQ4)
  - [【公式总结】条件概率及全概率公式及贝叶斯公式](http://1t.click/aAQ5)
  - [【教材】statistics for data science](http://1t.click/aAQ8)
